<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deploying a computer vision model | MORESCODE ANALYTICS LLC</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Deploying a computer vision model" />
<meta name="author" content="Paul Moresco" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Training and publishing a computer vision model on the web. Try It Here! üß™ Lightweight JavaScript demo üöÄ Full-featured Gradio app" />
<meta property="og:description" content="Training and publishing a computer vision model on the web. Try It Here! üß™ Lightweight JavaScript demo üöÄ Full-featured Gradio app" />
<link rel="canonical" href="http://localhost:4000/2025/07/01/deploying-computer-vision.html" />
<meta property="og:url" content="http://localhost:4000/2025/07/01/deploying-computer-vision.html" />
<meta property="og:site_name" content="MORESCODE ANALYTICS LLC" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-01T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deploying a computer vision model" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Paul Moresco"},"dateModified":"2025-07-01T00:00:00-05:00","datePublished":"2025-07-01T00:00:00-05:00","description":"Training and publishing a computer vision model on the web. Try It Here! üß™ Lightweight JavaScript demo üöÄ Full-featured Gradio app","headline":"Deploying a computer vision model","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/07/01/deploying-computer-vision.html"},"url":"http://localhost:4000/2025/07/01/deploying-computer-vision.html"}</script>
<!-- End Jekyll SEO tag -->
<link id="main-stylesheet" rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="MORESCODE ANALYTICS LLC" />
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/">MORESCODE ANALYTICS LLC</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/about/">About</a>
  <a class="nav-item" href="/portfolio.html">Portfolio</a>
</div>
      </nav>
  </div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="post"><h1 class="page-heading" style="text-align: center;">Deploying a computer vision model</h1><div class="post-meta" style="text-align: center; color: #666; margin-bottom: 1.5em;">
    Jul 1, 2025&nbsp;-&nbsp;<span class="post-tag" style="color: teal">#fastai</a>, <span class="post-tag" style="color: teal">#html</a>, <span class="post-tag" style="color: teal">#js</a>, <span class="post-tag" style="color: teal">#gradio</a>, <span class="post-tag" style="color: teal">#computervision</a>, <span class="post-tag" style="color: teal">#huggingface</a></div>

  <div class="post-content">
    <p>Training and publishing a computer vision model on the web.</p>
<h4 id="try-it-here">Try It Here!</h4>
<ul>
  <li><a href="/hfapi">üß™ Lightweight JavaScript demo</a></li>
  <li><a href="https://huggingface.co/spaces/morescode-pm/urbanrivers-camtraps">üöÄ Full-featured Gradio app</a></li>
</ul>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#training-a-computer-vision-model">Training a new model based on human labeled images</a>
    <ul>
      <li>Downloading images with labels</li>
      <li>Cleaning input images for model fine tuning</li>
      <li>Testing performance and iterating</li>
      <li>Saving and exporting the model</li>
    </ul>
  </li>
  <li><a href="#publishing-the-new-model-for-testing">Publishing and Predicting</a>
    <ul>
      <li>HF Gradio App</li>
      <li>Lightweight Javascript API</li>
    </ul>
  </li>
  <li><a href="#next-steps">Next Steps</a>
    <ul>
      <li>trim top and bottom of camera trap images to avoid the brand logos</li>
      <li>Crop (to crop or not to crop) images and try fine tuning speciesnet</li>
    </ul>
  </li>
</ol>

<h1 id="the-goal-why-do-this">The goal (why do this?)</h1>
<p>Urban Rivers aims to improve urban environments for animals and humans by revitalizing waterways with floating wetland installaions.
To understand the effect of installtations, they aim to study changes in wildlife and human use in and along the chicago river.
Changes in biodiversity and species abundance before and after the installation of floating wetlands would be evidence of an effect.
How to gather this data? At least in-part through observations made using motion capture cameras.</p>

<p>Why AI: Because motion capture cameras have a high false positive image capture rate ( nothing in the picture ). Early estimates have this false positive rate at ~90%. This is why using computer vision to accelerate data capture is a huge help. We‚Äôre also making use of volunteer labeling to train AI - because correct identification requires some specialized knowledge.</p>

<p>I previously attempted to use <em><code class="language-plaintext highlighter-rouge">SpeciesNet</code></em> to classify our images without any additional fine-tuning and had variable results. This time, I wanted to see what we could do with the images we have labeled so far - with an eye on getting more of the location specific categories correctly labeled.</p>

<h2 id="training-a-computer-vision-model">Training a computer vision model</h2>
<p>All training took place on the kaggle platform. I chose kaggle for easy access to a GPU based on the expected need for fine tuning/training.<br />
To get the images, I fed a long list of target species to the website‚Äôs api and parsed the JSON response. ChatGPT was very helpful for making lists and troubleshooting the api.</p>

<p>After getting metadata from 12118 image observations (images that had at least one observation) there were a few cleaning steps to do. At each step I rendered a snippet of the data to confrim what was happening was intentional. Cleaning involved deduplication, filling <em><code class="language-plaintext highlighter-rouge">None</code></em> values with ‚Äúblank‚Äù, making sure that at least 3 people made the same species observation, filtering out multiple species (a tougher problem) and keeping single species or blank only images.</p>

<p>At the end of cleaning we had 1228 images remaining (only about 10%) with only 4 categories having over 100 images, and a total of 9 categories with at least 26 pictures.<br />
Normally, we would want at least 100-200 images per class - but at this moment that would only let us train on canadian geese, ducks, spotted sandpipers, and blank images. So in order to keep things interesting I set a threshold of at least 26.</p>

<p><img src="/assets/images/fastai-deploy/1-image-counts.png" /></p>

<p>From there it was a matter of downloading each species into an images/species_name folder - this structure is popular for many deep learning applications and has support in the fastai infrastructure. The parent folder indicates the target class - so loading images into fastai‚Äôs dataloaders/datablocks can take advantage of the <code class="language-plaintext highlighter-rouge">parent_label</code> function at the same time as resizing and conducting some augmentation transforms.<br />
The <code class="language-plaintext highlighter-rouge">aug_transforms()</code> used essentially - while fine-tuning the model - provide some image alterations to expose the CNN to variations of the photos. It‚Äôs a great way to bolster your image numbers when you have small to medium datasets. Here‚Äôs a small panel that show some of the augments:</p>

<p><img src="/assets/images/fastai-deploy/2-dls-batch.png" /></p>

<p>I decided to try Resnet18 - because that‚Äôs in the example for the fastai documentation (#overly-honest-methods).<br />
Training started to converge at 5 epochs, so for the sake of iteration in the future I stopped there.  Here‚Äôs the confusion matrix from only 26 images each (5 held for validation). Not bad! - clearly some issues with blanks, sandpipers and ducks but that has to be expected with the small dataset.</p>

<p><img src="/assets/images/fastai-deploy/classify.png" /></p>

<p>Here‚Äôs a small prediction outcome panel from images witheld from training and validation (test images):</p>

<p><img src="/assets/images/fastai-deploy/res18x25_output.png" /></p>

<p>The lower right corner shows a good example of why goose and beaver might be hard to differentiate. The deployment is the same in those photos (camera location) and it also looks like the reveal logo is larger for that model of camera. This is exactly the reason that SpeciesNet version B expects some of the image trimmed on the top and bottom: it‚Äôs entirely possible that the model is using that portion of the image to make predictions (leakage). Our batch augmentation does remove a few of the issues, but in the future we should add cropping to the item transforms.</p>

<h5 id="full-cleaning-and-training-notebook-attached">Full Cleaning and Training Notebook attached:</h5>
<iframe src="/assets/notebooks/html/ur-resnet18-v7.html" width="100%" height="400" style="border:1px solid #ccc; border-radius:8px;"></iframe>

<h2 id="publishing-the-new-model-for-testing">Publishing the new model for testing</h2>
<p>The goal in this part of the project is just to learn what it would take to publish the new fine tuned model to the web for people to try.<br />
As luck would have it - this part is also free; thanks to hugging face spaces + gradio. First the app is developed in a jupyter notebook, then thanks to nbdev package - that notebook can be converted into an app.py. Huggingface spaces provides a lot of help for initializing the project, too.</p>

<p>Here‚Äôs the notebook used for development of the <a href="https://huggingface.co/spaces/morescode-pm/urbanrivers-camtraps">gradio app</a>:</p>
<iframe src="/assets/notebooks/html/gradio-app.html" width="100%" height="400" style="border:1px solid #ccc; border-radius:8px;"></iframe>

<p>Lastly - Huggingface Spaces also provide an API endpoint for running the gradio app - this includes a javascript implementation for a very <a href="/hfapi">lightweight deployment right here</a>.</p>

<h2 id="next-steps">Next Steps</h2>
<p>Our model, while achieving about 78% accuracy in the training and validation datasets, does a poor job of generalizing to new images.
To iterate and improve here‚Äôs a short list of what to do first.</p>
<ol>
  <li>Trim the top and bottom of images</li>
  <li>Striate the location/deployment (requires additional metadata)</li>
  <li>Examine images labeled by humans and by Speciesnet/Version 1 of the fine tuned model to refine the dataset used for tuning</li>
  <li>Try fine-tuning on the model version B of speciesnet</li>
</ol>

<h2 id="update">Update</h2>
<p>The original dataloaders object was being passed ‚ÄúResize(224)‚Äù - this was doing a center crop and removing the target species sometimes. By adding Resize(224, method=‚Äôsquish‚Äô) and a class for cropping the top and bottom - this was resolved while also removing the cameratrap overlays.</p>

<p>This, plus checking timm for state of the art models and switching to convnext_tiny improved the error rate from 22% to 11%!</p>

  </div>
</div>
      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/"></data>

  <div class="wrapper">
    <div class="footer-row" style="text-align: center;"><div class="footer-author" style="margin-bottom: 8px;">
          <span class="p-name">&copy; 2025 Paul Moresco </span>&nbsp;|&nbsp;
            <a class="u-email" href="mailto:hello@morescode-analytics.com">hello@morescode-analytics.com</a></div><div class="social-links"><ul class="social-media-list"><li>
    <a rel="me" href="https://github.com/morescode-pm" target="_blank" title="GitHub">
      <span class="grey fa-brands fa-github fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://www.linkedin.com/in/paul-moresco" target="_blank" title="Linkedin">
      <span class="grey fa-brands fa-linkedin fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://www.kaggle.com/morescope/" target="_blank" title="Kaggle">
      <span class="grey fa-brands fa-kaggle fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://public.tableau.com/app/profile/paul.moresco/vizzes" target="_blank" title="Tableau Public">
      <span class="grey fa-brands fa-salesforce fa-lg"></span>
    </a>
  </li>
</ul></div>
    </div>
  </div>
</footer></body>

</html>