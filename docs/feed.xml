<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-11T15:36:29-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">MORESCODE ANALYTICS LLC</title><subtitle>Data science, analytics, and statistical consulting.
</subtitle><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><entry><title type="html">Building a Webapp for Species Geofencing</title><link href="http://localhost:4000/2025/06/07/species-geofence.html" rel="alternate" type="text/html" title="Building a Webapp for Species Geofencing" /><published>2025-06-07T00:00:00-05:00</published><updated>2025-06-07T00:00:00-05:00</updated><id>http://localhost:4000/2025/06/07/species-geofence</id><content type="html" xml:base="http://localhost:4000/2025/06/07/species-geofence.html"><![CDATA[<p>One of the issues we experienced with speciesnet classification was results where the species was not valid for our location. In speciesnet - specifying a geofence coordinate will cause missclassifications to move up to the highest level hierarchy - in most cases, this classifies an animal as “animal.”</p>

<p>So, to help validate lists of species relevant to our area, I built a <a href="https://morescode-pm.github.io/geofence-polygon/">webapp for looking up species</a>.
<a href="https://morescode-pm.github.io/geofence-polygon/" target="_blank">
    <img src="/assets/images/geofence/1_previewapp.png" />
</a></p>

<h2 id="a-well-known-issue">A well known issue</h2>
<p>I ran a test of google’s species net and wrote about the <a href="/2025/05/22/camera-trap-computer-vision.html">results here</a>.</p>

<p>Species geofencing is problematic. Animals don’t seem to care where you draw lines on a map and they don’t have “guest books” where they can sign their names. We rely on human observation to tell us if a species is found in an area or not.</p>

<p>Of all the public resources for these observations - the <em>Global Biodiversity Information Facility (GBIF)</em> API has a great search query representation for polygons. This search is <a href="https://www.gbif.org/occurrence/search">baked into their tools</a>, but I wanted to be able to filter the results for unique species instead of by occurence.</p>

<p>The ability to draw on a map and be presented with a unique list of confirmed species was the main goal.</p>

<p>Cursor, Jules, and GitHub CoPilot created the majority of the code - we started in Flask and then I asked for a refactor for a full front end site for easy hosting (wow).</p>

<p>The project repo and branch can be found on my <a href="https://github.com/morescode-pm/geofence-polygon/tree/convert/only-frontend">GitHub</a>.</p>

<h2 id="cleaning-ai-computer-vision-results-based-on-polygon-geofence-occurrences">Cleaning AI computer vision results based on polygon geofence occurrences</h2>
<p>This polygon was used to then submit a query to the GBIF dataset saved on Google BigQuery.</p>

<pre><code class="language-SQL">WITH unique_species AS (
    SELECT DISTINCT
    class,
    `order`,
    family,
    genus,
    species,
    taxonkey

    FROM
    `bigquery-public-data.gbif.occurrences` 

    WHERE 
    ST_WITHIN(
        ST_GEOGPOINT(decimallongitude, decimallatitude),
        ST_GEOGFROMTEXT('POLYGON((
            -87.69081115722656 42.005312912238956, 
            -87.66952514648438 41.955818412264705, 
            -87.61596679687501 41.905774595463853, 
            -87.60910034179689 41.85779952612765, 
            -87.62626647949219 41.815801430687642, 
            -87.7196502685547 41.808127409160392, 
            -87.71690368652345 41.842908943268263, 
            -87.67982482910158 41.88533726561532, 
            -87.72377014160158 41.946119107705776, 
            -87.78625488281251 41.99051961904691, 
            -87.69081115722656 42.005312912238956
        ))')
    )
    AND LOWER(phylum) = "chordata" # This is the phylum that includes mammals and birds
    
    LIMIT 1000 
)

SELECT
  t1.*,
  ARRAY_AGG(DISTINCT countrycode IGNORE NULLS) AS country_codes

FROM unique_species t1
LEFT JOIN `bigquery-public-data.gbif.occurrences` 
USING(taxonkey)

GROUP BY 
  class,
  `order`,
  family,
  genus,
  species,
  taxonkey
</code></pre>

<p>Less than 1000 were returned in this query, but that could be a large bill if the polygon area were huge</p>

<h2 id="using-geofencing-for-validated-species-lists-vs-speciesnet">Using geofencing for validated species lists vs. speciesnet</h2>
<p>There are 3 datasets worth using to clean up missclassified speciesnet data:</p>
<ol>
  <li>The predictions dataset itself ( from our non-geofenced speciesnet inference on our images )</li>
  <li>The published SpeciesNet taxonomy_release.txt file ( a list of all possible output classifications )</li>
  <li>A geofenced dataset from the polygon created using the webapp ( filtering gbif occurrences by location )</li>
</ol>

<p>Each dataset has its own structure, but ultimately they share a listing of the class/order/family/genus/species heirarchy of taxonomic classification.<br />
Joining and filtering to find what species should be valid for a location is possible almost entirely in python-pandas.</p>

<iframe src="/assets/notebooks/html/species-mismatch.html" width="100%" height="400" style="border:1px solid #ccc; border-radius:8px;"></iframe>
<p>The notebook for this analysis can be <a href="https://github.com/morescode-pm/urbanrivers-speciesnet-preview/blob/store-and-combine-predicts/_speciesnet-taxa/species_mismatch_analysis.ipynb">found here</a>.</p>

<p>Example of matching vs non-matching lists:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| matching (speciesnet&amp;gbif-chi)   | non_matching (speciesnet!gbif-chi)    |
|----------------------------------|---------------------------------------|
| blank                            | human                                 |
| bird                             | western pond turtle                   |
| mallard                          | anseriformes order                    |
| american coot                    | domestic dog                          |
| northern raccoon                 | reptile                               |
| great blue heron                 | domestic cattle                       |
| vehicle                          | wild turkey                           |
| eastern cottontail               | central american agouti              |
| brown rat                        | white-tailed deer                     |
| domestic cat                     | nutria                                |
| muskrat                          | crocodile                             |
| wood duck                        | wild boar                             |
| coyote                           | mammal                                |
| canada goose                     | common tapeti                         |
| american beaver                  | tome's spiny rat                      |
| eastern gray squirrel            | ocellated turkey                      |
| domestic horse                   | rodent                                |
| american robin                   | collared peccary                      |
| white-crowned sparrow           | spotted paca                          |
| sylvilagus species              | madagascar crested ibis              |
| song sparrow                     | canis species                         |
| snowy egret                      | red squirrel                          |
| california quail                 | bushy-tailed woodrat                  |
| north american river otter       | pronghorn                             |
| eastern chipmunk                 | eastern red forest rat               |
| &lt;NA&gt;                             | domestic chicken                      |
| &lt;NA&gt;                             | blood pheasant                        |
| &lt;NA&gt;                             | white-lipped peccary                 |
| &lt;NA&gt;                             | red acouchi                           |
| &lt;NA&gt;                             | desert cottontail                     |
| &lt;NA&gt;                             | plains zebra                          |
| &lt;NA&gt;                             | rufescent tiger-heron                 |
| &lt;NA&gt;                             | owl                                   |
| &lt;NA&gt;                             | common wombat                         |
| &lt;NA&gt;                             | bearded pig                           |
| &lt;NA&gt;                             | fossa                                 |
| &lt;NA&gt;                             | nine-banded armadillo                 |
| &lt;NA&gt;                             | guenther's dik-dik                    |
</code></pre></div></div>

<h2 id="officially-not-in-illinois-at-all">Officially not in Illinois at all:</h2>

<pre>| common_name              | state_code                                           |
|--------------------------|------------------------------------------------------|
| bushy-tailed woodrat     | [CO, CA, WY, OR, UT, WA, NM, MT, SD, NV, ID, ...]     |
| central american agouti  | [TX]                                                 |
| common wombat            | [NM, WA]                                             |
| crocodile                | [FL]                                                 |
| desert cottontail        | [CA, AZ, CO, NM, TX, NV, UT, WY, MT, NE, SD, KS, ...] |
| fossa                    | [NE, TX]                                             |
| guenther's dik-dik       | [TX]                                                 |
| ocellated turkey         | [CA, FL, HI, OK]                                     |
| plains zebra             | [CA, NM, TX, WA, AK, CO, MA, OH, OR, UT]             |
| pronghorn                | [WY, CO, NM, UT, AZ, SD, MT, OR, ID, TX, NV, CO, ...] |
| red acouchi              | [NE, NY, WA]                                         |
| rufescent tiger-heron    | [HI]                                                 |
| spotted paca             | [CO, CA, RI, UT, WA]                                 |
| western pond turtle      | [CA, OR, NV]                                         |
| white-lipped peccary     | [TX, CO, TX]                                         |</pre>

<p>Note we didn’t apply any geofencing to this run of speciesnet - so it’s expected to return the highest confidence match without ‘rolling-up’ to the higher taxonomy level.</p>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="ai" /><category term="html" /><category term="js" /><category term="cursor" /><category term="jules" /><category term="claude" /><category term="gpt4o" /><summary type="html"><![CDATA[One of the issues we experienced with speciesnet classification was results where the species was not valid for our location. In speciesnet - specifying a geofence coordinate will cause missclassifications to move up to the highest level hierarchy - in most cases, this classifies an animal as “animal.”]]></summary></entry><entry><title type="html">Catch A Vibe Code</title><link href="http://localhost:4000/2025/06/03/catch-a-vibe-code.html" rel="alternate" type="text/html" title="Catch A Vibe Code" /><published>2025-06-03T00:00:00-05:00</published><updated>2025-06-03T00:00:00-05:00</updated><id>http://localhost:4000/2025/06/03/catch-a-vibe-code</id><content type="html" xml:base="http://localhost:4000/2025/06/03/catch-a-vibe-code.html"><![CDATA[<p>I joined an ai for conservation slack group recently. In that group, one of the developers of SpeciesNet (<a href="/2025/05/22/camera-trap-computer-vision.html">camera trap computer vision</a>) and the creator of the MegaDetector object detection model, <a href="https://github.com/agentmorris">Dan Morris</a>, hosted a “Vibe coding party” this week.</p>

<p>The tools for agentic coding are developing rapidly - the point of the party was to start learning and using them now, so that in 6 months we have a much better understanding of the landscape.</p>

<blockquote>
  <p>“If you are using AI for autocompletion, that’s great, but <strong>if you’re using only AI for autocompletion, you may be at a counterproductive local maximum.</strong>  I.e., you may be better off <em>not</em> using AI for autocomplete if it forces you to re-think how you use AI.” - Dan Morris</p>
</blockquote>

<p>So, this week I spent some time at the Vibe coding party and afterwards trying to see if AI could help me quickly contribute features to the website that Urban Rivers uses to have volunteers help label camera trap images – and then for fun, trying to make GAMES!</p>

<h2 id="using-jules-on-a-massive-code-base-for-urban-rivers-ranger-website">Using <a href="https://jules.google.com/">Jules</a> on a massive code base for Urban Rivers <a href="https://rangers.urbanrivers.org/">ranger website</a>.</h2>
<p>The wildmile codebase is &gt;45k lines of code - written in Next.js with MongoDB integrations. Between Frontend, Backend, Models, Actions, Utils, and Tests, and having not previously contributed to the project or spent any real time learning the languages - I’ve hit several barriers to being able to contribute.</p>

<p>We’ve wanted to expand our rewards and recognition systems to motivate people who are able to contribute daily or weekly - more than just once.  There’s already a system to add badges - which as an admin I’ve been able to add to. For example, Here’s one you get after labeling 9000! images along with 700 points - generated using chatGPT image mode:</p>

<p><img src="/assets/images/ai-part-one/1_exbadge.png" width="200" /></p>

<p>I have this idea that by logging in weekly, people will be ‘taking care of the river.’  Conceptually, we can create a ‘tomagachi-pal-like’ image of the river, and each week the picture shared gets prettier, until it looks like a very nice, boardwalk lined, wetlands environment.  This then could be picked up as a sticker from Urban Rivers - or mailed - or awarded at a social.</p>

<p>The existing system was struggling with login cookies so we decided to change it to an “actions” based system - where any valid action (labeling observations) within a day is considered an active day and can increase the streak.</p>

<p>Here’s what I prompted Jules with:</p>
<blockquote>
  <p>we have a series of cards that display the user progress and update the streak ex:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>UserProgressSchema.methods.updateStreak = function () { const today = new Date(); const lastLogin &gt;= this.streaks.lastLoginDate;

if (!lastLogin) { this.streaks.current = 1; } else { const daysSinceLastLogin = Math.floor( (today &gt;- lastLogin) / (1000 * 60 * 60 * 24) );

if (daysSinceLastLogin === 1) {
 this.streaks.current += 1;
 this.streaks.longest = Math.max(
   this.streaks.current,
   this.streaks.longest
 );
} else if (daysSinceLastLogin &gt; 1) {
 this.streaks.current = 1;
}
}

this.streaks.lastLoginDate = today; };
</code></pre></div>  </div>
  <p>Instead of using the login date, we want to create a function to update a parameter about the user based on their last action on our site - a valid action would be saving an observation. Please make suggestions on how to implement this</p>
</blockquote>

<p>From there, Jules read in the entire codebase (I provided credentials), built a plan to edit the function, edited the 3 very specific files needed to change the feature, and - in noticing our lack of tests - added steps on how to implement an entire testing ecosystem.</p>

<p>Here’s a screengrab of where it shows the changes suggested:<br />
<img src="/assets/images/ai-part-one/3_jules.png" /></p>

<p>I pulled the branch into my local env to test out the changes and it loaded. Now, the test system didn’t work at all - but I do plan on getting jules to help with that again in the near future.</p>

<p>As of today, Jules is using Google Gemini 2.5 Pro.</p>

<p>See the <a href="https://github.com/nkwsy/Wildmile/pull/269">commit</a> here!</p>

<h2 id="using-claude-sonnet-35-as-a-cli-agent-to-get-it-running">Using claude sonnet 3.5 as a CLI agent to get it running.</h2>
<p>When I first tried to run the website locally to see if my changes worked, I couldn’t get the system running at all. After several chat prompts with an AI - I did manage to get something running - only to realize that the wrong drive was being used and needing to start over.</p>

<p>There was also a problem the first time I tried to submit the PR - my fork of the project was more than 170 commits behind the master branch! This was a gap in my own understanding - when I was merging with the master, I was merging with MY master, not the original. To fix this, all I had to do was hit the “refresh” button on my repo page.</p>

<p>After all of that - pulling the changes and syncing my main branch into my local server - I experimented with Claude Sonnet 3.5 in copilot within VSCode. The difference here being that the CLI version of the AI is able to see what my computer is (Windows) and how incompatibility issues with linux were causing the problems. It fixed things instantly.</p>

<p><img src="/assets/images/ai-part-one/4_copilot_claude.png" /></p>

<p>Given, this may have only taken a few google searches, but it’s something Jules was incapable of knowing - because Jules is entirely online on a VM.</p>

<h2 id="using-copilot-and-claude-to-make-a-game-for-elephant-conservation">Using copilot and claude to make a game for elephant conservation.</h2>
<p>In the last 30 minutes of the vibe coding party I suggested we all try to use AI to make a game - for fun &amp; to see how ‘one-shot’ prompting works these days (this was one of the ideas Dan had originally conceived for the party).  The premise for the game came from another conservation member - <a href="https://www.linkedin.com/in/prabath/">Prabath Gunawardane</a> - who was interested in creating a game that helped teach how elephant friendly deterrents could help prevent farm damage.</p>

<p>Here was my <a href="https://github.com/morescode-pm/elephriend-zone/blob/main/development-chat-log.md">prompt and chat log</a> to claude:</p>
<blockquote>
  <p>I would like to make a game I can run in my local browser called “Elephriend Zone” - it’s a game where elephants will be drawn to farms nearby and could damage them - damaging the farms is a game over condition - the player can build elephant safe de-motivation items on the map - please look up some elephant friendly options for this. For art we can start with simple shapes. Pick any language that is known to be low bandwith and run on a slow computer.</p>
</blockquote>

<p>And here’s <a href="https://github.com/morescode-pm/elephriend-zone">the repo</a> &amp; screen shot of the..</p>
<h3 id="elephriend-zone-game-you-can-actually-play"><strong><a href="https://morescode-pm.github.io/elephriend-zone/">Elephriend Zone</a> game you can actually play</strong>:</h3>

<p><img src="/assets/images/ai-part-one/5_elephriends.png" /></p>

<h2 id="in-summary">In Summary</h2>
<p>The most incredible part of all of this was that I only had to (barely) review the code, and figure out testing/deployment strategies. When you think about the time it would take to learn new languages just to be able to make one small change (months). This is truly an amazing thing.</p>

<!-- links  -->]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="ai" /><category term="html" /><category term="next.js" /><category term="css" /><category term="copilot" /><summary type="html"><![CDATA[I joined an ai for conservation slack group recently. In that group, one of the developers of SpeciesNet (camera trap computer vision) and the creator of the MegaDetector object detection model, Dan Morris, hosted a “Vibe coding party” this week.]]></summary></entry><entry><title type="html">Converting from HTML was a project</title><link href="http://localhost:4000/2025/05/30/starting-from-scratch.html" rel="alternate" type="text/html" title="Converting from HTML was a project" /><published>2025-05-30T00:00:00-05:00</published><updated>2025-05-30T00:00:00-05:00</updated><id>http://localhost:4000/2025/05/30/starting-from-scratch</id><content type="html" xml:base="http://localhost:4000/2025/05/30/starting-from-scratch.html"><![CDATA[<p>Wow, making a website in Jekyll is great, but it definitely has a learning curve.<br />
Even after purchasing a domain, figuring out the DNS records for github, and getting an HTML page initially stood up there was still a lot to learn.</p>

<p>Here are the steps I followed on windows:</p>
<ol>
  <li>Download Ruby (with Bundler by default) and Jekyll on Windows
    <ul>
      <li>Here’s the <a href="https://rubyinstaller.org/downloads/">Ruby+Devkit</a> for windows install page - I had to use 3.3.8-1</li>
    </ul>
  </li>
  <li>Read a bunch of how to guides that are based on varying deployments of jekyll and jekyll themes
    <ul>
      <li>Here’s a link to the most comprehensive walkthrough when using <a href="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll">github pages with Jekyll</a>.</li>
    </ul>
  </li>
  <li>Double check that all software installed correctly in the Path variable.
    <ul>
      <li>This includes not installing the most up to date version of jekyll because github pages uses an older version. They will likely not update because of stability concerns for users.</li>
      <li>Confirm installs with <code class="language-plaintext highlighter-rouge">bundle exec jekyll -v</code>  and <code class="language-plaintext highlighter-rouge">ruby-v</code></li>
      <li>Very important Gemfile line: <code class="language-plaintext highlighter-rouge">gem "github-pages", "~&gt; 232", group: :jekyll_plugins</code></li>
    </ul>
  </li>
  <li>Spend a lot of time figuring out what the _config.yml file does and how to configure it.</li>
  <li>Learn about what <code class="language-plaintext highlighter-rouge">gems</code>, <code class="language-plaintext highlighter-rouge">bundler</code>, <code class="language-plaintext highlighter-rouge">_layouts</code> and <code class="language-plaintext highlighter-rouge">_includes</code> are for and what jinja syntax is being used in jekyll.</li>
  <li>More that I thought to myself “wow this would be good to write down” but didn’t.</li>
</ol>

<p><br />
What you get for all the effort:</p>
<ul>
  <li>Strong thematic elements across all pages.</li>
  <li>A templating implementation to avoid repeating yourself in html.</li>
  <li>The option to write posts in markdown and html - instead of all html.</li>
  <li>Examples from other users that you can use to improve your own site.</li>
  <li>An easy and auto-reloading localhost:4000 testing environment
    <ul>
      <li><code class="language-plaintext highlighter-rouge">bundle exec jekyll serve</code></li>
    </ul>
  </li>
  <li>Ultimately, the knowledge of how to do this again in the future and a fast way to add new projects.</li>
</ul>

<p><br />
Here’s a <a href="https://github.com/morescode-pm/portfolio/tree/gh-jekyll">link to the repo</a> where the project is stored.</p>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="jekyll" /><category term="ruby" /><category term="html" /><summary type="html"><![CDATA[Wow, making a website in Jekyll is great, but it definitely has a learning curve. Even after purchasing a domain, figuring out the DNS records for github, and getting an HTML page initially stood up there was still a lot to learn.]]></summary></entry><entry><title type="html">Learning PowerBI</title><link href="http://localhost:4000/2025/05/25/learning-powerbi.html" rel="alternate" type="text/html" title="Learning PowerBI" /><published>2025-05-25T00:00:00-05:00</published><updated>2025-05-25T00:00:00-05:00</updated><id>http://localhost:4000/2025/05/25/learning-powerbi</id><content type="html" xml:base="http://localhost:4000/2025/05/25/learning-powerbi.html"><![CDATA[<p>Plant Co. Performance Dashboard.<br />
An example dataset from a fake company that sells real plants.<br />
The dashboard is interactive, so please click through to satisfy your curiosity (tip: the embedded dashboard can be resized to fit your full screen in the lower right corner).</p>

<iframe title="Plant Co Performance Report" width="788" height="505" src="https://app.powerbi.com/view?r=eyJrIjoiOWJmMTRmMjgtNWNiNC00ODkxLWJjMTItMDEyYjc0YzNlOTRlIiwidCI6ImQxMjE2YWM4LWFiOGQtNDg0ZC1hOTg2LTlmMGRmMmMxMjBmMCIsImMiOjJ9&amp;pageName=2195c7825b6c3b144e2f" frameborder="0" allowfullscreen="true">
</iframe>

<p><br /></p>

<p><em>Example Analysis:</em></p>
<h2 style="margin-top: 0"> October '23 year to date was less profitable than '22 by $0.15M:</h2>
<p>Focusing on the Gross Profit Year to Date (YTD) vs Prior Year to Date (PYTD) Waterfall Chart for 2023.<br />
It looks like October was our worst performing month relative to last year - we want to know why so we can make decisions to increase profitability next year.</p>

<p>First, hover over and then click on the “Click to turn on Drill down” arrow (1). Then click on the red bar for October (2).<br />
<img src="/assets/images/learning-powerbi/1_DrillDown.png" /></p>

<p>This waterfall chart has drill down information about country and product - when clicking on October, the first layer of detail shows us a list of all countries and their relative gross profit between 2023 and 2022. Here we can see 79k of the $148.41k less profit in 2023 came from accounts in China. This is reflected across all visuals in the dashboard and hovering over points/bars will show you more information.
<img src="/assets/images/learning-powerbi/2_Countries.png" /></p>

<p>Drilling further down by clicking on china we get a grouping by product category: Indoor, Landscape, and Outdoor.<br />
It looks like in 2023 we made less money on Outdoor and Landscape products than in 2022 but are making small improvements on Indoor products profits. Remember, at this point we are drilled down on “Gross Profit YTD (2023) vs PYTD (2022) for October 2023, from China.”
<img src="/assets/images/learning-powerbi/3_ProductGroup.png" /></p>

<p>Focusing in further on Outdoor we can get an idea of which specific plants are experiencing lower gross profits in 2023. For example, 4 of these products had $9-10k less gross profits in 2023 than 2022 - something definitely worth looking into more. (and we would repeat this for Landscape).
<img src="/assets/images/learning-powerbi/4_Products.png" /></p>

<p>To get a final list of the products we can just request a table view by right clicking on the graph. Tip: Use portrait or landscape view to  (because, yeah, reading long names from angled bar titles isn’t great) 
<img src="/assets/images/learning-powerbi/5_Tabled.png" /></p>

<p>Remember - this does not mean the products <em>LOST</em> money, it just means relative to last year they made less. So, maybe the products are falling out of favor in china, or maybe in 2022 there was an issue with these. It could also mean that the products are more perrenial, and in 2022 companies bought all the needed. So, when thinking about the next year we could update the catalog to offer products that are in more demand and produce less of the products out of demand for the chinese market.<br />
We could also look deeper at the account profitability segments, and try to advertize to the accounts related that are low value YTD, but high GP% (bottom right figure of the dashboard).</p>

<p><br /></p>

<p><em>How this was made:</em></p>
<h2 style="margin-top: 0"> Project Introduction and Data Cleaning </h2>
<p>This project was created following Mo Chen’s Power BI <a href="https://www.youtube.com/watch?v=BLxW9ZSuuVI&amp;t=92s">Tutorial</a>.<br />
The goal was to create a performance reporting dashboard for a company that would dynamically interact with user input. This type of tool is invaluable to businesses looking to better understand their sales and strategize for the future.</p>

<p>The Steps involved were:</p>
<ol>
  <li>Data gathering (power query, virtual tables)</li>
  <li>Modeling - Measures, calculated columns</li>
  <li>Visuals - Layout and Design</li>
  <li>Review and Publishing</li>
</ol>

<h3 id="data-sources"><em>Data sources:</em></h3>
<ul>
  <li>Excel Workbook with 3 Sheets:
    <ol>
      <li>Plant FACT (date, product id, quantity, price, cogs, account id)</li>
      <li>Accounts (account id, country, master id, lat/lon, postal code, street name)</li>
      <li>Plant_Hierarchy (product family, group, name, size, type)</li>
    </ol>
  </li>
</ul>

<p><em>Power Query:</em></p>
<ul>
  <li>Data was imported into Power Query for Transformation.</li>
  <li>Column names were cleaned and deduplication was run on any primary keys.</li>
  <li>Table names were cleaned and identified as FACT or DIM tables</li>
</ul>

<h3 id="data-model"><em>Data Model:</em></h3>
<ul>
  <li>New Table for Dates - This will be a major way we slice the data for analysis. Includes Date Hierarchies!
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  DIM_Date =  
  CALENDAR(  
      DATE(2022,01,01),  
      DATE(2024,12,31)  
  )
</code></pre></div>    </div>
  </li>
  <li>A boolean to check if the FACT_Sales row is before or after the sliced date (will be true for PYTD)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  inPast = 
  VAR lastsalesdate = MAX(FACT_Sales[Date_Time]) # Finds the most recent sales date in FACT_Sales
  VAR lastsalesdatePY = EDATE(lastsalesdate, -12) # Calculates same date 12 months earlier
  RETURN
  DIM_Date[Date]&lt;=lastsalesdatePY # TRUE for any date in DIM_Date that is before latest sales date from prior year
</code></pre></div>    </div>
  </li>
  <li>Slicers:<br />
  Table of Slicer Values as “Enter Data” Type<br />
  “Sales”, “Gross Profit”, “Quantity</li>
  <li>Joins
    <ul>
      <li>FACT_Sales.Account_id = DIM_Account.Account_id</li>
      <li>FACT_Sales.Product_id = DIM_Product.Product_Name_id</li>
      <li>FACT_Sales.Date_Time = DIM_Date.Date</li>
    </ul>
  </li>
</ul>

<p><em>DAX Measures:</em> (always applies to the interval/date selected)<br />
<code class="language-plaintext highlighter-rouge">COGs = SUM(FACT_Sales[COGS_USD])</code>  # Cost of goods sold summed<br />
<code class="language-plaintext highlighter-rouge">Quantity = SUM(FACT_Sales[quantity])</code> # Quantity of units sold<br />
<code class="language-plaintext highlighter-rouge">Sales = SUM(FACT_Sales[Sales_USD])</code> # Sales amount<br />
<code class="language-plaintext highlighter-rouge">Gross Profit = [Sales] - [COGs]</code> # How much is made before EBITDA</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- PYTD_GrossProfit =
  CALCULATE( # shifts filter context to the same period in the last year (ex oct 22 if looking at oct 23)
    [Gross Profit],
    SAMEPERIODLASTYEAR(DIM_Date[Date]),
    DIM_Date[inPast] = TRUE
  )
- PYTD_Quantity = 
  CALCULATE(
    [Quantity],
    SAMEPERIODLASTYEAR(DIM_Date[Date]),
    DIM_Date[inPast] = TRUE
  )
- PYTD_Sales = 
  CALCULATE(
    [Sales],
    SAMEPERIODLASTYEAR(DIM_Date[Date]),
    DIM_Date[inPast] = TRUE
  )
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- YTD_GrossProfit = 
  TOTALYTD( # Sums the gross profit from the start of the year to the current context
    [Gross Profit],
    FACT_Sales[Date_Time]
  )
- YTD_Quantity = 
  TOTALYTD(
    [Quantity],
    FACT_Sales[Date_Time]
  )
- YTD_Sales = 
  TOTALYTD(
    [Sales],
    FACT_Sales[Date_Time]
  )
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- S_PYTD = 
  VAR selected_value = SELECTEDVALUE(SLC_Values[Values])
  VAR result = 
  SWITCH(
      selected_value,
      "Sales", [PYTD_Sales],
      "Quantity", [PYTD_Quantity],
      "Gross Profit", [PYTD_GrossProfit],
      BLANK()
  )
  RETURN result

- S_YTD = 
  VAR selected_value = SELECTEDVALUE(SLC_Values[Values])
  VAR result = 
  SWITCH(
      selected_value,
      "Sales", [YTD_Sales],
      "Quantity", [YTD_Quantity],
      "Gross Profit", [YTD_GrossProfit],
      BLANK()
  )
  RETURN result
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- YTD vs PYTD = [S_YTD] - [S_PYTD]
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- GP% = DIVIDE(
    [Gross Profit],
    [Sales]
)
</code></pre></div></div>

<h3 id="visualizations"><em>Visualizations:</em></h3>
<ul>
  <li>Treemap chart - Month and Country by YTD vs PTYD</li>
  <li>Waterfall chart - Month, Country, Type, Name by YTD vs PYTD</li>
  <li>Line and Stacked Column chart - Quarter/Month by Value YTD (col) and Value PTYD (line) by Type</li>
  <li>Scatter chart - Value YTD vs GP% by Account - with segmentation through average lines</li>
</ul>

<h3 id="layout-and-final-touches"><em>Layout and Final Touches:</em></h3>
<ul>
  <li>Sort Measures by YTD or PYTD</li>
  <li>Organize Measure Switches Folder</li>
  <li>Build a background in powerpoint and use it as a PowerBI background image.</li>
  <li>Transparent backgrounds, rounded corners, conditional formatting</li>
  <li>Final fixes on titles and dynamic titles (uses <code class="language-plaintext highlighter-rouge">SELECTEDVALUE(SLC_Values[Values])</code>)
  _Column Title
  _Report Title
  _Scatter Title
  _Waterfall Title</li>
</ul>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="powerbi" /><category term="dax" /><summary type="html"><![CDATA[Plant Co. Performance Dashboard. An example dataset from a fake company that sells real plants. The dashboard is interactive, so please click through to satisfy your curiosity (tip: the embedded dashboard can be resized to fit your full screen in the lower right corner).]]></summary></entry><entry><title type="html">Analyzing Camera Trap Pictures with AI</title><link href="http://localhost:4000/2025/05/22/camera-trap-computer-vision.html" rel="alternate" type="text/html" title="Analyzing Camera Trap Pictures with AI" /><published>2025-05-22T00:00:00-05:00</published><updated>2025-05-22T00:00:00-05:00</updated><id>http://localhost:4000/2025/05/22/camera-trap-computer-vision</id><content type="html" xml:base="http://localhost:4000/2025/05/22/camera-trap-computer-vision.html"><![CDATA[<p>Using SpeciesNet (AI) to sort through hundreds of thousands of wildlife pictures from motion detection cameras (camera traps).</p>

<h3 id="skip-to-the-good-part">Skip to the good part:</h3>
<p>Preview page created using Megadetector dev-tools on a subset of predictions.<br />
<a href="https://morescode-pm.github.io/urbanrivers-speciesnet-preview/">Urban Rivers - CamTrap AI Observations</a></p>

<p><a href="#obligatory-example-of-good-classification">A good example of successfull <strong>Great Blue Heron</strong> classification</a></p>

<h3 id="project-workflow">Project Workflow</h3>
<ol>
  <li>Process Images through the <code class="language-plaintext highlighter-rouge">SpeciesNet</code> Deep Learning Computer Vision Model.</li>
  <li>Associate inferences back to images via md5 hash - Data Cleaning.</li>
  <li>Explore Dataset - EDA.</li>
  <li>Plan for model fine-tuning &amp; next steps.</li>
</ol>

<h3 id="notebook-for-processing-images-on-kaggle">Notebook for processing images on <a href="https://www.kaggle.com/code/morescope/urbanrivers-speciesnet-hash-clean-full-nogeo">Kaggle</a>:</h3>
<iframe src="/assets/notebooks/html/ur-speciesnet-v2.html" width="100%" height="400" style="border:1px solid #ccc; border-radius:8px;"></iframe>
<p>The notebook took 10 hours to run - 7 hours of which was just spent on downloading and resizing images. This is why I used batching and multi-threading for the images, but yet it still takes a while to go through 100k s3 links. Surely there’s a better way.</p>

<h3 id="data-cleaning-and-eda">Data Cleaning and EDA</h3>
<p>Our primary goal with image classification - before releasing a model - is to confirm that predictions are “accurate enough”. This generally takes shape by having a holdout set of images with known (human graded) classes the ai doesn’t have access to. In our situation, we don’t have this quite yet - images are being labeled and classifications are on the production server - but the speciesnet ensemble doesn’t yet have a fine-tuning or base truth approach listed.</p>

<p>Ultimately this would let us “vote” on identifying species in photos using AI.</p>

<p>Since this is the first time we’re trying speciesnet, we’re going to do a few spot checks and EDA of the generated inference (predicted animals in photo) to understand how it performs.</p>

<h4 id="cleaning">Cleaning</h4>
<p>After running the model we are left with a very long json file of 100k+ classifications and a pre-assembled [html pages][1] site through using <code class="language-plaintext highlighter-rouge">megadetector-utils</code> 
Here’s an example of the structure for one image where a great blue heron was classified:</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"filepath"</span><span class="p">:</span><span class="w"> </span><span class="s2">"images/batch_5/bada276600ce2790b1d2877e7d6e7a25.jpg"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"classifications"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"classes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="s2">"96fe1a07-7ef1-4a2f-99e1-ec2c9a78b532;aves;pelecaniformes;ardeidae;ardea;herodias;great blue heron"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"bfa75aeb-3187-48fe-95b9-f171465cc984;aves;pelecaniformes;ardeidae;ardea;cocoi;cocoi heron"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"b1352069-a39c-4a84-a949-60044271c0c1;aves;;;;;bird"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"f8db21f0-6b79-4444-8be4-b87906d56e6a;aves;pelecaniformes;ardeidae;ardea;albus;great egret"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"1110460b-7f99-405b-a9b0-65a09ecccca1;aves;pelecaniformes;ardeidae;tigrisoma;lineatum;rufescent tiger-heron"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"scores"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="mf">0.7359567880630493</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.07253273576498032</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.04953608289361</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.03795544430613518</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.008134812116622925</span><span class="w">
    </span><span class="p">]</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"detections"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
        </span><span class="nl">"category"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"animal"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"conf"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8977401256561279</span><span class="p">,</span><span class="w">
        </span><span class="nl">"bbox"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.3460410535335541</span><span class="p">,</span><span class="w">
        </span><span class="mf">0.9453125</span><span class="w">
        </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"prediction"</span><span class="p">:</span><span class="w"> </span><span class="s2">"96fe1a07-7ef1-4a2f-99e1-ec2c9a78b532;aves;pelecaniformes;ardeidae;ardea;herodias;great blue heron"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"prediction_score"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7359567880630493</span><span class="p">,</span><span class="w">
    </span><span class="nl">"prediction_source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"classifier"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"model_version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"4.0.1a"</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>First, we need to extract data from the JSON file into a dataframe so we can more easily parse through some questions. Looking at the JSON structure, we need the filepath, a paired list of the classifications and their scores, and a paired list of any detections and their scores. Speciesnet does ultimately give us a prediction inference based on a taxonomic hierarchy decision set, but to learn more, we want the full dataset minus some very low confidence prections. My preference for this step is to stick to pure python for parsing - aiming to create a list of dicts for generating a dataframe. In my opinion, this is a lot easier to read than the jargon in the pandas library for extraction.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_CONF</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># Master Setting for confidence or score threshold
</span>
<span class="c1"># Function for pairing classification and score
</span><span class="k">def</span> <span class="nf">get_high_score_classes</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">_CONF</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[{</span><span class="s">"score"</span><span class="p">:</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="s">"class"</span><span class="p">:</span> <span class="n">cls</span><span class="p">}</span> <span class="k">for</span> <span class="n">score</span><span class="p">,</span> <span class="n">cls</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span> <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>

<span class="c1"># Function for pairing detection and score
</span><span class="k">def</span> <span class="nf">get_high_conf_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">,</span> <span class="n">target_label</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">_CONF</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">det</span><span class="p">[</span><span class="s">"conf"</span><span class="p">]</span><span class="si">:</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">,</span> <span class="n">det</span><span class="p">[</span><span class="s">"label"</span><span class="p">])</span> <span class="k">for</span> <span class="n">det</span> <span class="ow">in</span> <span class="n">detections</span> <span class="k">if</span> <span class="n">det</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span> <span class="o">==</span> <span class="n">target_label</span> <span class="ow">and</span> <span class="n">det</span><span class="p">[</span><span class="s">"conf"</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>

<span class="c1"># Staged Configuration
</span><span class="n">master_data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Open and loop over json file
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">master_json</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s">"predictions"</span><span class="p">]:</span>
        <span class="n">high_score_classes</span> <span class="o">=</span> <span class="n">get_high_score_classes</span><span class="p">(</span>
            <span class="n">image</span><span class="p">[</span><span class="s">"classifications"</span><span class="p">][</span><span class="s">"scores"</span><span class="p">],</span>
            <span class="n">image</span><span class="p">[</span><span class="s">"classifications"</span><span class="p">][</span><span class="s">"classes"</span><span class="p">],</span>
            <span class="n">_CONF</span>
        <span class="p">)</span>
        <span class="n">master_data</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s">"imageName"</span><span class="p">:</span> <span class="n">image</span><span class="p">[</span><span class="s">"filepath"</span><span class="p">].</span><span class="n">split</span><span class="p">(</span><span class="s">"/"</span><span class="p">)[</span><span class="mi">2</span><span class="p">],</span>
            <span class="s">"nDetections_animal"</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_high_conf_detections</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="s">"detections"</span><span class="p">],</span> <span class="s">"animal"</span><span class="p">,</span> <span class="n">_CONF</span><span class="p">)),</span>
            <span class="s">"nDetections_human"</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_high_conf_detections</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="s">"detections"</span><span class="p">],</span> <span class="s">"human"</span><span class="p">,</span> <span class="n">_CONF</span><span class="p">)),</span>
            <span class="s">"nDetections_vehicle"</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_high_conf_detections</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="s">"detections"</span><span class="p">],</span> <span class="s">"vehicle"</span><span class="p">,</span> <span class="n">_CONF</span><span class="p">)),</span>
            <span class="s">"nClassifications"</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">high_score_classes</span><span class="p">),</span>
            <span class="s">"highScoreClasses"</span><span class="p">:</span> <span class="n">high_score_classes</span>

        <span class="p">})</span>

<span class="c1"># Previous work identified 55363 as our heron example index - 
</span><span class="n">display</span><span class="p">(</span><span class="n">master_data</span><span class="p">[</span><span class="mi">55363</span><span class="p">])</span>

</code></pre></div></div>
<p>This script processes all 100k+ predictions in about 7 seconds, extracting a preferable starting point for EDA.<br />
The heron example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'imageName'</span><span class="p">:</span> <span class="s">'bada276600ce2790b1d2877e7d6e7a25.jpg'</span><span class="p">,</span>
 <span class="s">'nDetections_animal'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'nDetections_human'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="s">'nDetections_vehicle'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
 <span class="s">'nClassifications'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'highScoreClasses'</span><span class="p">:</span> <span class="p">[{</span>
    <span class="s">'score'</span><span class="p">:</span> <span class="s">'0.74'</span><span class="p">,</span>
    <span class="s">'class'</span><span class="p">:</span> <span class="s">'96fe1a07-7ef1-4a2f-99e1-ec2c9a78b532;aves;pelecaniformes;ardeidae;ardea;herodias;great blue heron'</span>
 <span class="p">}]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>From here it’s very simple to just define the dataframe as <code class="language-plaintext highlighter-rouge">df = pd.DataFrame(master_data)</code></p>

<p>Next, we want to make sure that each row in our dataframe contains exactly one class prediction. This step is important because if an image has more than one classification above our threshold, while we’re ok with having more than one classification per image (sometimes there are more than 1 species in a photo!), we want to know the high score classification(s) in each photo.</p>

<p>For example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imageName</span><span class="p">:</span> <span class="s">'269ed4dcbaf7419a1d00e6957ff2beff.jpg'</span>
<span class="n">highScoreClasses</span><span class="p">:</span>
  <span class="p">[</span>
    <span class="p">{</span><span class="s">'score'</span><span class="p">:</span> <span class="s">'0.57'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">:</span> <span class="s">'f1856211-cfb7-4a5b-9158-c0f72fd09ee6;;;;;;blank'</span><span class="p">},</span>
    <span class="p">{</span><span class="s">'score'</span><span class="p">:</span> <span class="s">'0.37'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">:</span> <span class="s">'d372cda5-a8ca-4b7b-97ed-4e4fab9c9b4b;mammalia;cetartiodactyla;suidae;sus;scrofa;wild boar'</span><span class="p">}</span>
  <span class="p">]</span>
</code></pre></div></div>

<p>With <code class="language-plaintext highlighter-rouge">df2 = df.explode("highScoreClasses", ignore_index=True)</code> turns into:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imageName</span><span class="p">:</span> <span class="s">'269ed4dcbaf7419a1d00e6957ff2beff.jpg'</span>
<span class="n">highScoreClasses</span><span class="p">:</span>
  <span class="p">[</span>
    <span class="p">{</span><span class="s">'score'</span><span class="p">:</span> <span class="s">'0.57'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">:</span> <span class="s">'f1856211-cfb7-4a5b-9158-c0f72fd09ee6;;;;;;blank'</span><span class="p">}</span>
  <span class="p">]</span>
<span class="n">imageName</span><span class="p">:</span> <span class="s">'269ed4dcbaf7419a1d00e6957ff2beff.jpg'</span>
<span class="n">highScoreClasses</span><span class="p">:</span>
  <span class="p">[</span>
    <span class="p">{</span><span class="s">'score'</span><span class="p">:</span> <span class="s">'0.37'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">:</span> <span class="s">'d372cda5-a8ca-4b7b-97ed-4e4fab9c9b4b;mammalia;cetartiodactyla;suidae;sus;scrofa;wild boar'</span><span class="p">}</span>
  <span class="p">]</span>
</code></pre></div></div>
<p>This increased our row count from 104937 to 107535</p>

<p>Now we’re going to parse that very long taxonomic string into the common name ‘most specific’ name that occupies the last item in the semicolon separated list. This is a very long line of code - but, chatGPT helped puzzle through it and we’re going to use it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split by ';' and take the last part
</span><span class="n">df2</span><span class="p">[</span><span class="s">'class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s">'highScoreClasses'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">d</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'class'</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="bp">None</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">";"</span><span class="p">).</span><span class="nb">str</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>Last, we want to re-associate some of the data originally pulled for the predictions - namely the datetime and the publicURL for each image. To do that we will merge the predictions with the metadata - both files were downloaded as CSVs at this stage or early on in the process and can be linked via the unique mediaID (a simple parse of the imageName).</p>

<p><code class="language-plaintext highlighter-rouge">merged_df = pd.merge(metadata, predicts, on="mediaID", how="inner")</code></p>

<p>After running this the first time, the merged_df had a few hundred more rows than the predictions dataset - so, the metadata probably has duplicated media ids. This is pretty normal because the image uploading process doesn’t include deduplication. So, before the merge I added <code class="language-plaintext highlighter-rouge">metadata_unique = metadata.drop_duplicates(subset=['mediaID'])</code></p>

<p><code class="language-plaintext highlighter-rouge">merged_df = pd.merge(metadata_unique, predicts, on="mediaID", how="inner")</code></p>

<p>Selecting our needed columns gives us:</p>

<h4 id="cleaned-data">Cleaned Data</h4>

<table>
  <thead>
    <tr>
      <th>mediaID</th>
      <th>timestamp</th>
      <th>publicURL</th>
      <th>folderName</th>
      <th>animal</th>
      <th>human</th>
      <th>vehicle</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>113f9bb5…39b0</td>
      <td>2024-12-25 15:29:24</td>
      <td>https://urbanriverrangers…SYFW01868.jpg</td>
      <td>2025-01-04_UR027</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>human</td>
    </tr>
    <tr>
      <td>871315ba…d6de</td>
      <td>2024-12-25 15:29:24</td>
      <td>https://urbanriverrangers…SYFW01869.jpg</td>
      <td>2025-01-04_UR027</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>human</td>
    </tr>
    <tr>
      <td>991baede…a9db</td>
      <td>2024-12-25 15:29:24</td>
      <td>https://urbanriverrangers…SYFW01870.jpg</td>
      <td>2025-01-04_UR027</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>human</td>
    </tr>
    <tr>
      <td>32c8912b…f68c</td>
      <td>2024-12-25 15:29:26</td>
      <td>https://urbanriverrangers…SYFW01871.jpg</td>
      <td>2025-01-04_UR027</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>blank</td>
    </tr>
    <tr>
      <td>f4ba4a3d…8d0a</td>
      <td>2024-12-25 15:29:26</td>
      <td>https://urbanriverrangers…SYFW01872.jpg</td>
      <td>2025-01-04_UR027</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>blank</td>
    </tr>
  </tbody>
</table>

<h4 id="exploratory-analysis">Exploratory Analysis</h4>
<p>First let’s just get a list of the top 20 classifications and graph it - there is an overwhelming number of blanks - so we’re going to use a log-scale.<br />
<img src="/assets/images/ai-part-one/6_bargraph_freq.png" />
..and by the numbers:</p>

<table>
  <thead>
    <tr>
      <th>#</th>
      <th>Class</th>
      <th>Count</th>
      <th>%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>blank</td>
      <td>90475</td>
      <td>84.1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>bird</td>
      <td>6122</td>
      <td>5.7</td>
    </tr>
    <tr>
      <td>3</td>
      <td>human</td>
      <td>4499</td>
      <td>4.2</td>
    </tr>
    <tr>
      <td>4</td>
      <td>western pond turtle</td>
      <td>1886</td>
      <td>1.8</td>
    </tr>
    <tr>
      <td>5</td>
      <td>mallard</td>
      <td>818</td>
      <td>0.76</td>
    </tr>
    <tr>
      <td>6</td>
      <td>anseriformes order</td>
      <td>588</td>
      <td>0.55</td>
    </tr>
    <tr>
      <td>7</td>
      <td>american coot</td>
      <td>299</td>
      <td>0.28</td>
    </tr>
    <tr>
      <td>8</td>
      <td>domestic dog</td>
      <td>161</td>
      <td>0.15</td>
    </tr>
    <tr>
      <td>9</td>
      <td>northern raccoon</td>
      <td>151</td>
      <td>0.14</td>
    </tr>
    <tr>
      <td>10</td>
      <td>reptile</td>
      <td>150</td>
      <td>0.14</td>
    </tr>
    <tr>
      <td>11</td>
      <td>great blue heron</td>
      <td>128</td>
      <td>0.12</td>
    </tr>
    <tr>
      <td>12</td>
      <td>vehicle</td>
      <td>109</td>
      <td>0.10</td>
    </tr>
    <tr>
      <td>13</td>
      <td>eastern cottontail</td>
      <td>82</td>
      <td>0.076</td>
    </tr>
    <tr>
      <td>14</td>
      <td>wild turkey</td>
      <td>68</td>
      <td>0.063</td>
    </tr>
    <tr>
      <td>15</td>
      <td>domestic cattle</td>
      <td>66</td>
      <td>0.061</td>
    </tr>
    <tr>
      <td>16</td>
      <td>wood duck</td>
      <td>53</td>
      <td>0.049</td>
    </tr>
    <tr>
      <td>17</td>
      <td>brown rat</td>
      <td>50</td>
      <td>0.046</td>
    </tr>
    <tr>
      <td>18</td>
      <td>white-tailed deer</td>
      <td>43</td>
      <td>0.040</td>
    </tr>
    <tr>
      <td>19</td>
      <td>central american agouti</td>
      <td>41</td>
      <td>0.038</td>
    </tr>
    <tr>
      <td>20</td>
      <td>domestic cat</td>
      <td>38</td>
      <td>0.035</td>
    </tr>
  </tbody>
</table>

<p>An overwhelming 84% of the images are blanks according to speciesnet. Given probably more than a few of these are just false negatives, but still - it mirror’s what we’ve been experiencing in labeling activities: a whole lot of boring.</p>

<p>For completeness - let’s also look at the least frequent 20. Most of these are ties because they are n=1:</p>

<table>
  <thead>
    <tr>
      <th>#</th>
      <th>Class</th>
      <th>Count</th>
      <th>%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>65</td>
      <td>rattus species</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>66</td>
      <td>greater roadrunner</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>67</td>
      <td>scaly ground-roller</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>68</td>
      <td>merle dyal malgache</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>69</td>
      <td>red acouchi</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>70</td>
      <td>blood pheasant</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>71</td>
      <td>white-lipped peccary</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>72</td>
      <td>red fox</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>73</td>
      <td>desert cottontail</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>74</td>
      <td>rufescent tiger-heron</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>75</td>
      <td>common wombat</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>76</td>
      <td>african elephant</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>77</td>
      <td>bearded pig</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>78</td>
      <td>eastern chipmunk</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>79</td>
      <td>fossa</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>80</td>
      <td>l’hoest’s monkey</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>81</td>
      <td>house rat</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>82</td>
      <td>bobcat</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>83</td>
      <td>nine-banded armadillo</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>84</td>
      <td>guenther’s dik-dik</td>
      <td>1</td>
      <td>0.001</td>
    </tr>
  </tbody>
</table>

<p>There are clearly some errors in classification. This run of the model was run without any geofencing to avoid collapsing low classification confidences into a higher “animal” taxonomy level. Probably a large portion of these would not exist. Increasing our _CONF threshold to &gt;0.80 reduced our species counts to 27 for example instead of 84. Unfortunately playing with this value drops some true detections in favor of having none at all - we will explore this type of decision more after we have an opportunity for fine-tuning.</p>

<p>Another popular angle in conservation is looking for patterns in usage by time of day. Here’s a chart for the top 10 classes and all of our pictures.<br />
This could be further subsetted into seasons but for this first pass we are just getting an understanding.</p>

<p><img src="/assets/images/ai-part-one/7_timeofday.png" /></p>

<p>There are definiely nocturnal patterns already emerging in a “we aren’t totally confident it got it right all that often” dataset.<br />
For example, we see humans follow a normal bell curve pattern of visitation, dogs seem to spike in the morning, noon, and evening (for walkies I hope), and species like cottontails and raccoons show up when the lights go out and nobody is around.</p>

<p>Same plot but with a shared y-axis:</p>

<p><img src="/assets/images/ai-part-one/8_timeofday_sharey.png" /></p>

<p>Here we get a sense of who we’re sharing the wild mile with: birds and turtles (not western pond turtles, but we’ll fix that soon)</p>

<p>The full cleaning and eda notebook is embedded in all of its’ <a href="https://github.com/morescode-pm/urbanrivers-speciesnet-preview/blob/store-and-combine-predicts/runs/full-no-geo/eda-predict-full-no-geo.ipynb">chaotic glory here</a>:</p>

<h4 id="obligatory-example-of-good-classification">Obligatory example of good classification</h4>
<p><a href="#skip-to-the-good-part">Link back to top</a></p>
<div style="max-width: 900px; display: flex; flex-wrap: wrap; gap: 10px;">
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-06-27_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/101MEDIA/SYFW0478.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/101MEDIA/SYFW0478.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-06-27_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW4008.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW4008.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-06-27_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW3285.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW3285.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-06-27_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW3099.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/100MEDIA/SYFW3099.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-10-12_UR005| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-10-12_UR005/DCIM/100DSCIM/PICT0534.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-10-12_UR005/DCIM/100DSCIM/PICT0534.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-06-27_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/101MEDIA/SYFW0467.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-06-27_UR011/DCIM/101MEDIA/SYFW0467.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-09-08_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-09-08_UR011/DCIM/100MEDIA/SYFW5409.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-09-08_UR011/DCIM/100MEDIA/SYFW5409.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-10-12_UR005| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-10-12_UR005/DCIM/100DSCIM/PICT0389.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-10-12_UR005/DCIM/100DSCIM/PICT0389.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
    
    <div style="flex: 1 0 30%; text-align: left;">
        <div style="margin-top: 4px; font-size: 12px; color: #ccc">2024-09-08_UR011| class: great blue heron</div>
        <a href="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-09-08_UR011/DCIM/100MEDIA/SYFW4971.JPG" target="_blank">
            <img src="https://urbanriverrangers.s3.amazonaws.com/images/2024/2024-09-08_UR011/DCIM/100MEDIA/SYFW4971.JPG" style="max-width: 100%; height: auto; max-height: 150px; border: 1px solid #ccc;" />
        </a>
    </div>
</div>

<h3 id="next-steps">Next Steps</h3>
<p>Overall, we succeeded in using the SpeciesNet ensemble released by google. We were able to generate over 100k predictions for our camera trap images, and parse those predictions into a useable structure for analysis and validation.</p>

<p>The first attempts at this used a gps geofence parameter and full resolution images. We seemed to have better luck by resizing the images before inference - something that speciesnet does also do - but perhaps the two stage downsampling of the images led to a more familiar (lower resolution) image like those used for training initially.</p>

<p>The next step I’m excited about involve learning how to deploy this model to huggingface through gradio &amp; then, after getting human graded image ids, working on fine tuning SpeciesNet to our image types.</p>

<p>The goal here is to reduce the boring parts of image labeling - something MegaDetector and SpeciesNet did excellently.</p>

<p>To get started on parsing the false positive (species who don’t belong) I build a webapp to interact with gbif data.
This is something you can both <a href="https://morescode-pm.github.io/geofence-polygon/">use here</a>.<br />
And <a href="/2025/06/07/species-geofence.html">read about here</a>.</p>

<p>SpeciesNet was described by Gadot et al.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Gadot, T., Istrate, Ș., Kim, H., Morris, D., Beery, S., Birch, T., &amp; Ahumada, J. (2024). <em>To crop or not to crop: Comparing whole-image and cropped classification on a large dataset of camera trap images.</em> IET Computer Vision. Wiley Online Library. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="deeplearning" /><category term="computervision" /><category term="opencv" /><category term="ai" /><category term="conservation" /><summary type="html"><![CDATA[Using SpeciesNet (AI) to sort through hundreds of thousands of wildlife pictures from motion detection cameras (camera traps).]]></summary></entry><entry><title type="html">Data Science Salaries</title><link href="http://localhost:4000/2025/04/30/data-science-salaries.html" rel="alternate" type="text/html" title="Data Science Salaries" /><published>2025-04-30T00:00:00-05:00</published><updated>2025-04-30T00:00:00-05:00</updated><id>http://localhost:4000/2025/04/30/data-science-salaries</id><content type="html" xml:base="http://localhost:4000/2025/04/30/data-science-salaries.html"><![CDATA[<p>Finding myself in an unscheduled sabatical / job search I decided to upskill in Looker while reviewing Data Science Salaries in Chicago.</p>

<p>When asked really early, it’s easy to point to this and ask for a range within the 25pct to 75pct band of the listed salary or industry salaries.</p>

<p>Here’s the final dashboard:</p>

<iframe width="788" height="505" src="https://lookerstudio.google.com/embed/reporting/ab71852c-1d0d-4cb2-8146-0c20aff7b331/page/page_12345" frameborder="0" style="border:0" allowfullscreen="" sandbox="allow-storage-access-by-user-activation allow-scripts allow-same-origin allow-popups allow-popups-to-escape-sandbox"></iframe>

<h2 id="how-it-was-made">How it was made</h2>
<p>First I built a structured table in google sheets for each job I was interested and applied for.<br />
This included:<br />
job title, company, location, pay, listed_years_experience, connections, and key_skills_listed</p>

<p><img src="/assets/images/salaries/1_table.png" /></p>

<p>I then parsed that information just a little further by cleaning the pay ranges into minimum and maximum values. <code class="language-plaintext highlighter-rouge">=SPLIT(pay, "-")</code></p>

<p>Looker studio is a free version of Looker that has direct access to google sheets data and is accessible via the extensions menu.</p>

<p><img src="/assets/images/salaries/2_looker_connect.png" /></p>

<p>This forges a direct connection to the google sheets page &amp; allows you to just “refresh” your looker visual when new rows are added to the table. Pretty nice for being totally free.</p>

<p>All we needed here was a simple visual to display the salary ranges.<br />
To get a sense of their relative bands, I went with a box plot chart for the quartiles in each listed salary range.<br />
<br />
This required some fiddling with the settings but I eventually found that I could create some calculated fields and use the box plot chart. Since the data I’m using are just 1 row per salary, I needed to use a bit of DAX to generate my metrics.</p>

<p>For each 25 pctile: <code class="language-plaintext highlighter-rouge">((High Salary-Low Salary)*&lt;pctile&gt;)+Low Salary</code><br />
Then, the dimensions for drilling down I had were <code class="language-plaintext highlighter-rouge">sector</code> (A field I inferred from the title manually in the google sheet) and <code class="language-plaintext highlighter-rouge">Company, Title</code> another DAX field just doing: <code class="language-plaintext highlighter-rouge">CONCAT(Company,", ",Title)</code>.</p>

<p>Here’s what the final setup looked like:<br />
<img src="/assets/images/salaries/3_setup.png" /></p>

<p>Next we added some <strong>Style</strong> by adding reference lines and bands for the full visual.  Meaning each salary belongs to the set of salaries, and we can add a band for the average 25pct and 75pct (the interquartile range or IQR) for the entire dataset.  This plus my last base salary as a parameter added some context to the visual.</p>

<p>What I like most about the finished product is the ability to quickly see a breakdown for the full dataset as well as for each industry that I labeled. Here’s that drilled up view.</p>

<p><img src="/assets/images/salaries/4_drillup.png" /></p>

<p>In the visual we can see how tech salaries are generally higher, and food and public sectors are lower. Part of the reason for this is the bias I have on jobs that look interesting, but I think the trend would hold relatively well.</p>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="jobs" /><category term="looker" /><category term="dax" /><summary type="html"><![CDATA[Finding myself in an unscheduled sabatical / job search I decided to upskill in Looker while reviewing Data Science Salaries in Chicago.]]></summary></entry><entry><title type="html">Blueberry Chihuahuas</title><link href="http://localhost:4000/2025/03/04/blueberry-chihuahuas.html" rel="alternate" type="text/html" title="Blueberry Chihuahuas" /><published>2025-03-04T00:00:00-06:00</published><updated>2025-03-04T00:00:00-06:00</updated><id>http://localhost:4000/2025/03/04/blueberry-chihuahuas</id><content type="html" xml:base="http://localhost:4000/2025/03/04/blueberry-chihuahuas.html"><![CDATA[<p>I decided to learn how to interact with computer vision models using the FastAI infrastructure.<br />
The coursework is taught by Jeremy Howard - former President and Chief Scientist at Kaggle and long time AI leader.</p>

<p>Here’s the embedded notebook for the first lesson (also available <a href="https://www.kaggle.com/code/morescope/is-it-a-chihuahua-or-blueberry-muffin">directly on Kaggle</a>).<br />
The goal was to learn how to use the fastai toolkit for a simple image classification problem.<br />
I chose a difficult task for humans to do: differentiate between chihuahuas and blueberry muffins.</p>

<iframe src="/assets/notebooks/html/blueberry-chihuahua.html" width="100%" height="900" style="border:1px solid #ccc; border-radius:8px;"></iframe>]]></content><author><name>Paul Moresco</name><email>hello@morescode-analytics.com</email></author><category term="deeplearning" /><category term="computervision" /><category term="pytorch" /><category term="ai" /><summary type="html"><![CDATA[I decided to learn how to interact with computer vision models using the FastAI infrastructure. The coursework is taught by Jeremy Howard - former President and Chief Scientist at Kaggle and long time AI leader.]]></summary></entry></feed>